[{"title":"Kuberbetes CI/CD 持续集成Spring Cloud","date":"2018-09-13T03:05:56.000Z","path":"2018/09/13/Kuberbetes-CI-CD-持续集成Spring-Cloud/","text":"写在前面在现在这个科技不断快速发展的时代，我们产品的快速迭代变得十分重要，每个公司都有自己的迭代计划，有的是一个礼拜一个版本迭代，有的是半个月，也有的是更加频繁的迭代，面对这些迭代我们如何快速的平滑的发布应用就产生了 CI/CD，即持续集成和持续部署。 本场 Chat 会从零开始教会大家如何将我们的应用持续的，分场景、环境的持续的部署到 Kubernetes 集群中，以及我们的应用如何在不同的场景下，平滑的升级，以及构建我们自己的 Docker 私有仓库、代码托管服务、Jenkins 构建服务。 本场 Chat 将学到如下内容： Docker 私有仓库搭建，以及 Kubernetes 中使用私有仓库； 使用 Docker 搭建代码托管服务（ GitLab）； 搭建 Kubernetes 1.11.2版的的基础集群和可视化管理； 搭建 Jenkins 构建服务，利用 Pipeline 对应用镜像编译发布； 如何将以上服务串联起来组成一个完整的持续集成的流水线； Jenkins 构建服务如何构建不同分支上的代码，并发布到不同的环境中； 一些常见问题的解决办法。 我们先来看看实现这个持续集成的流水线： 开发人员提交代码 Gitlab 远程触发 Jenkins 构建 Jenkins 构建镜像推送到私有仓库 Jenkins 将最新的镜像部署到 k8s 中 将部署反馈发送给用户 由于GitChat版权限制，这篇实战教程不能免费发于个人博客中，这篇文章定价5元，总共77页。所以希望有需要的朋友可以去GitChat支持一下信客，信客在这里谢过各位大大了。 教程目录： 实验环境概览 虚拟机和必要软件安装 搭建 Docker 私有仓库：Harbor 搭建代码托管服务：GitLab 搭建 Jenkins 服务 搭建 k8s 集群 搭建完整的持续的流水线 常见问题和常用命令 效果预览 GitChat地址","tags":[{"name":"Docker","slug":"Docker","permalink":"https://blog.githink.cn/tags/Docker/"},{"name":"kubernetes","slug":"kubernetes","permalink":"https://blog.githink.cn/tags/kubernetes/"},{"name":"k8s","slug":"k8s","permalink":"https://blog.githink.cn/tags/k8s/"},{"name":"CI/CD","slug":"CI-CD","permalink":"https://blog.githink.cn/tags/CI-CD/"},{"name":"SpringCloud","slug":"SpringCloud","permalink":"https://blog.githink.cn/tags/SpringCloud/"}]},{"title":"kubernetes 安装 helm","date":"2018-09-13T02:46:49.000Z","path":"2018/09/13/kubernetes-安装-helm/","text":"1. Helm 客户端安装12345678# 1.官网下载二进制安装包wget https://storage.googleapis.com/kubernetes-helm/helm-v2.9.1-linux-amd64.tar.gz# 2.开始安装tar -zxvf helm-2.9.0.tar.gz# 3.移动到系统目录下mv helm-2.9.0/helm /usr/local/bin/helm 2. Helm 服务端安装1234567891011121314151617181920212223242526272829303132333435# 1.下载国内tiller镜像docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.9.1# 2.为每个节点安装 socatyum install -y socat# 3.使用Service Account安装## 1.创建一个名为tiller的Service Accountkubectl create serviceaccount tiller --namespace kube-system## 2.授予名为tiller的ServiceAccount集群管理员角色cluster-admin：vim helm-rbac-config.yaml### 加入如下内容：apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: tiller roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: tiller namespace: kube-system ### 授予tiller集群管理员角色： kubectl create -f helm-rbac-config.yaml ## 3.安装服务端（tiller） ### 创建服务端 helm init --service-account tiller --upgrade -i registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.9.1 --stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts ### 创建TLS认证服务端，参考地址：https://github.com/gjmzj/kubeasz/blob/master/docs/guide/helm.md helm init --service-account tiller --upgrade -i registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.9.1 --tiller-tls-cert /etc/kubernetes/ssl/tiller001.pem --tiller-tls-key /etc/kubernetes/ssl/tiller001-key.pem --tls-ca-cert /etc/kubernetes/ssl/ca.pem --tiller-namespace kube-system --stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts 4. 验证1234helm version### 结果：Client: &amp;version.Version&#123;SemVer:\"v2.9.1\", GitCommit:\"20adb27c7c5868466912eebdf6664e7390ebe710\", GitTreeState:\"clean\"&#125;Server: &amp;version.Version&#123;SemVer:\"v2.9.1\", GitCommit:\"20adb27c7c5868466912eebdf6664e7390ebe710\", GitTreeState:\"clean\"&#125; 5. Helm使用5.1. 更换仓库123456# 1.移除原来的仓库helm repo remove stable# 2.添加新仓库helm repo add stable https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts# 3.更新仓库helm repo update 5.2. 查看仓库中charts1helm search 5.3. 更新charts列表1helm repo update 5.4.安装chartsMonocular是一个开源软件，用于管理kubernetes上以Helm Charts形式创建的服务，可以通过它的web页面来安装helm Charts 5.4.1 安装Nginx Ingress controller，安装的k8s集群启用了RBAC，则一定要加rbac.create=true参数1helm install stable/nginx-ingress --set controller.hostNetwork=true，rbac.create=true 5.4.2 安装Monocular1234567891011121314151617181920# 1.添加新的源helm repo add monocular https://kubernetes-helm.github.io/monocular# 2.安装helm install monocular/monocular -f custom-repos.yaml# custom-repos.yaml 内容cat custom-repos.yamlapi: config: repos: - name: stable url: https://aliacs-app-catalog.oss-cn-hangzhou.aliyuncs.com/charts source: https://github.com/kubernetes/charts/tree/master/stable - name: incubator url: https://aliacs-app-catalog.oss-cn-hangzhou.aliyuncs.com/charts-incubator source: https://github.com/kubernetes/charts/tree/master/incubator - name: monocular url: https://kubernetes-helm.github.io/monocular source: https://github.com/kubernetes-helm/monocular/tree/master/charts 5.4.3 查看需要安装的values1helm inspect values &#123;chart名&#125; 5.5 查看K8S中已安装的charts1helm list 5.6 删除安装的charts1# 删除：helm delete xxx 6. 卸载Helm服务端1helm reset 或 helm reset --force 7. 参考地址： 1.https://github.com/gjmzj/kubeasz/blob/master/docs/guide/helm.md2.https://blog.csdn.net/wenwenxiong/article/details/790670543.https://blog.csdn.net/luanpeng825485697/article/details/808732364.https://blog.csdn.net/qq_35959573/article/details/80885052","tags":[{"name":"容器","slug":"容器","permalink":"https://blog.githink.cn/tags/容器/"},{"name":"kubernetes","slug":"kubernetes","permalink":"https://blog.githink.cn/tags/kubernetes/"},{"name":"k8s","slug":"k8s","permalink":"https://blog.githink.cn/tags/k8s/"},{"name":"helm","slug":"helm","permalink":"https://blog.githink.cn/tags/helm/"}]},{"title":"Centos 7 搭建 Docker 私服","date":"2018-09-13T02:45:12.000Z","path":"2018/09/13/Centos-7-搭建-Docker-私服/","text":"1.环境说明 系统 Ip Docker版本 Registry版本 Centos 7 10.0.0.149 18.06.0-ce 2.0 2. 安装 Docker-ce123456789101112131415161718192021222324#1.安装依赖包sudo yum install -y yum-utils device-mapper-persistent-data lvm2#2.设置阿里云镜像源sudo yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo#3.重建 Yum 缓存yum -y install epel-releaseyum clean allyum makecache#4.安装Docker-cesudo yum install docker-ce# 5.安装指定版本yum install docker-ce-17.09.0.ce -yyum install -y --setopt=obsoletes=0 docker-ce-17.03.2.ce-1.el7.centos.x86_64 docker-ce-selinux-17.03.2.ce-1.el7.centos.noarch#5.启动 Docker-cesudo systemctl enable dockersudo systemctl start docker#6.Docker建立用户组sudo groupadd dockersudo usermod -aG docker $USER 配置阿里云加速器12# 7.重新加载配置systemctl daemon-reload 3. 系统环境配置12345678910#1.关闭防火墙systemctl stop firewalld &amp;&amp; systemctl disable firewalld#2.关闭selinuxsetenforce 0#修改\"SELINUX=disabled\"为\"SELINUX=disabled\"vim /etc/selinux/config#3.修改hostnamevim /etc/hosts 4.搭建私服12345678910111213141516171819202122232425262728# 1.pull 官方registry镜像docker pull registry# 2. 添加 Volumemkdir /docker/registry# 3. 运行 registrydocker run -d -p 5000:5000 -v /docker/registry:/var/lib/registry --name docker-registry --restart always registry# 4.查看运行中的容器docker ps# 5.添加Tag标记# 拉取一个官方的Java镜像docker pull java:8-jre# 添加tagdocker tag java:8-jre docker-registry:5000/java:8-jre# 查看镜像docker images# 6.推送镜像到私服docker push docker-registry:5000/java:8-jre# 7.查看是否推送成功curl -XGET http://docker-registry:5000/v2/_catalog# 8.获取某个镜像的标签列表curl -XGET http://docker-registry:5000/v2/image_name/tags/list 5.常见问题1.Get https://docker-registry:5000/v1/_ping: http: server gave HTTP response to HTTPS client12345vim /etc/docker/daemon.json# 加入如下内容：&#123; \"insecure-registries\":[\"hub.k8s.com\"] &#125;# 重启服务systemctl restart docker 2.Error response from daemon: driver failed programming external connectivity on endpoint quirky_allen12# 重启服务systemctl restart docker","tags":[{"name":"Docker","slug":"Docker","permalink":"https://blog.githink.cn/tags/Docker/"},{"name":"容器","slug":"容器","permalink":"https://blog.githink.cn/tags/容器/"}]},{"title":"Centos 7 部署Kubernetes v1.11.1","date":"2018-09-13T02:42:47.000Z","path":"2018/09/13/Centos-7-部署Kubernetes-v1-11-1/","text":"1. 环境说明 系统 IP 名称 centos7 10.0.0.147 k8s-master centos7 10.0.0.146 k8s-slave1 centos7 10.0.0.148 k8s-slave2 2. 准备工作2.1 安装Docker123456789101112131415161718192021#1.安装依赖包sudo yum install -y yum-utils device-mapper-persistent-data lvm2&lt;!-- more --&gt;#2.设置阿里云镜像源sudo yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo#3.重建 Yum 缓存yum -y install epel-releaseyum clean allyum makecache#4.安装Docker-cesudo yum install docker-ce#5.启动 Docker-cesudo systemctl enable dockersudo systemctl start docker#6.Docker建立用户组sudo groupadd dockersudo usermod -aG docker $USER 配置阿里云加速器123# 重新加载配置systemctl daemon-reloadsystemctl restart docker 2.2 关闭防火墙和selinux1234567891011121314151617181920212223#1.关闭防火墙systemctl stop firewalld &amp;&amp; systemctl disable firewalld#2.关闭selinuxsetenforce 0#修改\"SELINUX=disabled\"为\"SELINUX=disabled\"vim /etc/selinux/config#3.关闭Swapsudo swapoff -a#要永久禁掉swap分区，打开如下文件注释掉swap那一行 sudo vi /etc/fstab#4.配置转发参数cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.confnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1vm.swappiness=0EOFsysctl --system#5.修改hostnamevim /etc/hosts 2.3 安装kudeadm、kubectl、kubelet、kubernetes-cni1234567891011121314151617181920212223242526272829303132333435#1. 配置阿里云软件源cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64enabled=1gpgcheck=0EOF#2.重建Yum缓存yum -y install epel-releaseyum clean allyum makecache#3.安装 kudeadmyum -y install kubelet kubeadm kubectl kubernetes-cni#4.启用kubeadm服务，并配置开机启动systemctl enable kubelet &amp;&amp; systemctl start kubelet#5.编写脚本下载需要的镜像（原镜像被墙）#!/bin/bashimages=(kube-proxy-amd64:v1.11.0 kube-scheduler-amd64:v1.11.0 kube-controller-manager-amd64:v1.11.0 kube-apiserver-amd64:v1.11.0etcd-amd64:3.2.18 coredns:1.1.3 pause-amd64:3.1 kubernetes-dashboard-amd64:v1.8.3 k8s-dns-sidecar-amd64:1.14.9 k8s-dns-kube-dns-amd64:1.14.9k8s-dns-dnsmasq-nanny-amd64:1.14.9 )for imageName in $&#123;images[@]&#125; ; dodocker pull keveon/$imageNamedocker tag keveon/$imageName k8s.gcr.io/$imageNamedocker rmi keveon/$imageNamedone# 个人新加的一句，V 1.11.0 必加docker tag da86e6ba6ca1 k8s.gcr.io/pause:3.1#6.修改脚本权限chmod -R 777 ./xxx.sh 注：这里我就遇到过一个坑，原作者是根据 1.10 来的，然后在 kubeadm init 执行的时候一直报错，说找不到镜像。之后镜像版本是下载对了，但还是在 [init] this might take a minute or longer if the control plane images have to be pulled 这一句卡住，在国外的 VPS 测试之后，发现多了一个 k8s.gcr.io/pause:3.1 镜像，他的 ID 其实与 pause-amd64:3.1 一样，然后加了一个新的 TAG 之后，正常部署。 3. Master 安装kubernetes12#1.初始化 Masterkubeadm init --kubernetes-version=v1.11.0 --pod-network-cidr=10.244.0.0/16 --apiserver-advertise-address=0.0.0.0 --apiserver-cert-extra-sans=&#123;Master Ip&#125;,127.0.0.1,k8s-master –kubernetes-version -&gt; 版本号；–pod-network-cidr -&gt; POD 网络的 IP 段；–apiserver-advertise-address -&gt;通过该 ip 地址向集群其他节点公布 api server 的信息，必须能够被其他节点访问； –apiserver-cert-extra-sans -&gt; 指定自己内网地址和映射；12345678910111213141516#2.配置 kubectl 认证信息export KUBECONFIG=/etc/kubernetes/admin.conf# 如果你想持久化的话，直接执行以下命令【推荐】echo \"export KUBECONFIG=/etc/kubernetes/admin.conf\" &gt;&gt; ~/.bash_profilemkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config#3. 查看初始化状态kubectl get cs#4.查看 Nodes 状态kubectl get nodes#5. 记录 Token 和 SShkubeadm join --token &lt;TOKEN&gt; --discovery-token-ca-cert-hash sha256:&lt;SSH&gt; 4. 安装 Flannel 网络123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169#依次执行以下命令mkdir -p /etc/cni/net.d/cat &lt;&lt;EOF&gt; /etc/cni/net.d/10-flannel.conf&#123;“name”: “cbr0”,“type”: “flannel”,“delegate”: &#123;“isDefaultGateway”: true&#125;&#125;EOFmkdir /usr/share/oci-umount/oci-umount.d -pmkdir /run/flannel/cat &lt;&lt;EOF&gt; /run/flannel/subnet.envFLANNEL_NETWORK=10.244.0.0/16FLANNEL_SUBNET=10.244.1.0/24FLANNEL_MTU=1450FLANNEL_IPMASQ=trueEOF# 创建 flannel.yml:---kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: flannelrules: - apiGroups: - \"\" resources: - pods verbs: - get - apiGroups: - \"\" resources: - nodes verbs: - list - watch - apiGroups: - \"\" resources: - nodes/status verbs: - patch---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: flannelroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: flannelsubjects:- kind: ServiceAccount name: flannel namespace: kube-system---apiVersion: v1kind: ServiceAccountmetadata: name: flannel namespace: kube-system---kind: ConfigMapapiVersion: v1metadata: name: kube-flannel-cfg namespace: kube-system labels: tier: node app: flanneldata: cni-conf.json: | &#123; \"name\": \"cbr0\", \"type\": \"flannel\", \"delegate\": &#123; \"isDefaultGateway\": true &#125; &#125; net-conf.json: | &#123; \"Network\": \"10.244.0.0/16\", \"Backend\": &#123; \"Type\": \"vxlan\" &#125; &#125;---apiVersion: extensions/v1beta1kind: DaemonSetmetadata: name: kube-flannel-ds namespace: kube-system labels: tier: node app: flannelspec: template: metadata: labels: tier: node app: flannel spec: hostNetwork: true nodeSelector: beta.kubernetes.io/arch: amd64 tolerations: - key: node-role.kubernetes.io/master operator: Exists effect: NoSchedule serviceAccountName: flannel initContainers: - name: install-cni image: quay.io/coreos/flannel:v0.9.1-amd64 command: - cp args: - -f - /etc/kube-flannel/cni-conf.json - /etc/cni/net.d/10-flannel.conf volumeMounts: - name: cni mountPath: /etc/cni/net.d - name: flannel-cfg mountPath: /etc/kube-flannel/ containers: - name: kube-flannel image: quay.io/coreos/flannel:v0.9.1-amd64 command: [ \"/opt/bin/flanneld\", \"--ip-masq\", \"--kube-subnet-mgr\" ] securityContext: privileged: true env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace volumeMounts: - name: run mountPath: /run - name: flannel-cfg mountPath: /etc/kube-flannel/ volumes: - name: run hostPath: path: /run - name: cni hostPath: path: /etc/cni/net.d - name: flannel-cfg configMap: name: kube-flannel-cfg#执行：kubectl create -f ./flannel.yml#查看节点信息kubectl get nodes 5.Dashboard 配置5.1 安装 GIT1yum -y install git 5.2 下载配置文件Kuberentes 配置 DashBoard 也不简单，当然你可以使用官方的 dashboard 的 yaml 文件进行部署，也可以使用 Mr.Devin 这位博主所提供的修改版，避免踩坑。 地址在：https://github.com/gh-Devin/kubernetes-dashboard，将这些 Yaml 文件下载下来，在其目录下（注意在 Yaml 文件所在目录），执行以下命令：1kubectl -n kube-system create -f . 启动 Dashboard 所需要的所有容器。 访问 MASTER 主机的 IP:30090，可以看到如下界面： image 5.3 配置权限创建dashboard-admin.yaml文件1vim dashboard-admin.yaml 然后填充如下内容：1234567891011121314apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: kubernetes-dashboard labels: k8s-app: kubernetes-dashboardroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects:- kind: ServiceAccount name: kubernetes-dashboard namespace: kube-system 行如下命令启动容器:1kubectl -f ./dashboard-admin.yaml create 刷新页面 image 6.配置 Slave 节点6.1 Slave 节点准备工作 重复第2步 准备工作 6.2 将子节点加入 Master将初始化 Master 节点时保存的Token信息在各个Node 节点上执行1kubeadm join 10.0.0.147:6443 --token l9phbw.q2atway8la8o8t09 --discovery-token-ca-cert-hash sha256:8b3e15ff633a5394767f64885ac4525a25f608e46ba90dd1218e0676ff0c76bd 配置kubectl 命令123sudo cp /etc/kubernetes/kubelet.conf $HOME/sudo chown $(id -u):$(id -g) $HOME/kubelet.confexport KUBECONFIG=$HOME/kubelet.conf ==主意：==必须配置kubectl命令，不然会出现以下错误:The connection to the server localhost:8080 was refused - did you specify the right host or port?(单独开终端也会出现）。 6.3 查看 Nodes12#在Master 节点上kubectl get nodes 将会看见所有节点的状态为Ready。 7. 常见问题 重新初始化Master 节点或者 各个子节点1kubeadm reset Master需要重新执行第3步及以下步骤，子节点需要重新执行第6步。 子节点加入Master时 Token 过期 默认token的有效期为24小时，当过期之后，该token就不可用了。解决方法如下：12345678#1. 重新生成Tokenkubeadm token create#2.获取ca证书sha256编码hash值openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&gt;/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //'#3. 子节点加入集群kubeadm join 10.0.0.147:6443 --token &lt;New Token&gt; --discovery-token-ca-cert-hash sha256:&lt;New SSH&gt; The connection to the server localhost:8080 was refused - did you specify the right host or port? 在该节点机器上执行如下命令:123sudo cp /etc/kubernetes/kubelet.conf $HOME/sudo chown $(id -u):$(id -g) $HOME/kubelet.confexport KUBECONFIG=$HOME/kubelet.conf 4.子节点加入不到Master，或者子节点访问不了 Api Server 初始化Master的时候需要指定 Api Server 能够被访问的IP 0.0.0.0 是任何IP等能反问。 8. 常用命令kubectl get cs //查看集群信息 kubectl get componentstatuses //查看node节点组件状态 kubectl get svc -n kube-system //查看应用 kubectl cluster-info //查看集群信息 kubectl cluster-info dump //更详细的集群信息 kubectl describe –namespace kube-system service kubernetes-dashboard //详细服务信息 kubectl apply -f kube-apiserver.yaml //更新kube-apiserver容器 kubectl delete -f /root/k8s/k8s_images/kubernetes-dashboard.yaml //删除应用 kubectl delete service example-server //删除服务 systemctl start kube-apiserver.service //启动服务。 kubectl get deployment –all-namespaces //启动的应用 kubectl get pod -o wide –all-namespaces //查看pod上跑哪些服务 kubectl get pod -o wide -n kube-system //查看应用在哪个node上 kubectl describe pod –namespace=kube-system //查看pod上活动信息 kubectl describe depoly kubernetes-dashboard -n kube-system kubectl get depoly kubernetes-dashboard -n kube-system -o yaml kubectl get service kubernetes-dashboard -n kube-system //查看应用 kubectl delete -f kubernetes-dashboard.yaml //删除应用 kubectl get events //查看事件 kubectl get rc/kubectl get svc kubectl get namespace //获取namespace信息 kubectl delete node 节点名 //删除节点 kubectl create namespace {名称} //创建namespace kubectl config set-context $(kubectl config current-context) –namespace={名称} //设置当前namespace kubectl get service,deployment,pod hostnamectl –static set-hostname host名称 9.参考资料 https://www.cnblogs.com/myzony/p/9298783.html https://www.cnblogs.com/yue-hong/p/8894033.html https://www.cnblogs.com/menkeyi/p/7128809.html","tags":[{"name":"Docker","slug":"Docker","permalink":"https://blog.githink.cn/tags/Docker/"},{"name":"容器","slug":"容器","permalink":"https://blog.githink.cn/tags/容器/"},{"name":"kubernetes","slug":"kubernetes","permalink":"https://blog.githink.cn/tags/kubernetes/"},{"name":"k8s","slug":"k8s","permalink":"https://blog.githink.cn/tags/k8s/"}]},{"title":"Docker 可视化 Portainer.io配置","date":"2018-09-13T02:38:57.000Z","path":"2018/09/13/Docker-可视化-Portainer-io配置/","text":"1.下载镜像123docker search portainerdocker pull portainer/portainer 2.单机运行Portainer.io123docker volume create portainer_datadocker run -d -p 9000:9000 --name portainer --restart always -v /etc/localtime:/etc/localtime -v /var/run/docker.sock:/var/run/docker.sock -v portainer_data:/data portainer/portainer 3.Swarm集群运行123docker volume create portainer_datadocker service create --name portainer --publish 9000:9000 --replicas=1 --constraint 'node.role == manager' --mount type=bind,src=//var/run/docker.sock,dst=/var/run/docker.sock --mount type=volume,src=portainer_data,dst=/data portainer/portainer -H unix:///var/run/docker.sock 4.常见问题在可视化界面添加docker端点的时候ping不通。1234# 1.在需要被添加的机器上修改docker.service（centos 7）vim /usr/lib/systemd/system/docker.service# 2.在 `ExecStart`属性后面追加如下内容：-H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock 5.参考文档 1.https://portainer.io/install.html 2.https://blog.csdn.net/u011781521/article/details/80469804","tags":[{"name":"Docker","slug":"Docker","permalink":"https://blog.githink.cn/tags/Docker/"},{"name":"容器","slug":"容器","permalink":"https://blog.githink.cn/tags/容器/"}]},{"title":"k8s高可用集群V1.11.1安装记录","date":"2018-09-13T02:23:54.000Z","path":"2018/09/13/k8s高可用集群V1-11-1安装记录/","text":"实验环境 hostname IP 内存 职责 k8s-vip 192.168.1.110 —— VIP（虚拟IP） k8s-master1 192.168.1.111 4G Master,Keepalived,HAProxy k8s-master2 192.168.1.112 4G Master,Keepalived,HAProxy k8s-master3 192.168.1.113 4G Master,Keepalived,HAProxy k8s-slave1 192.168.1.114 4G Worker k8s-slave1 192.168.1.115 4G Worker 环境准备1. 配置hosts信息（所有节点）12345678cat &lt;&lt;EOF &gt;&gt; /etc/hosts192.168.1.110 k8s-vip192.168.1.111 k8s-master1192.168.1.112 k8s-master2192.168.1.113 k8s-master3192.168.1.114 k8s-slave1192.168.1.115 k8s-slave2EOF 2. 安装Docker （所有节点）安装Docker，步骤略可参考http://www.ebanban.com/?p=496 3. 在所有Master节点上输入以下环境变量，主机名和IP信息根据自己的实际的情况进行修改（#台Master节点）12345678910export KUBECONFIG=/etc/kubernetes/admin.confexport LOAD_BALANCER_DNS=k8s-vipexport LOAD_BALANCER_PORT=8443export CP0_HOSTNAME=k8s-master1export CP1_HOSTNAME=k8s-master2export CP2_HOSTNAME=k8s-master3export VIP_IP=192.168.1.110export CP0_IP=192.168.1.111export CP1_IP=192.168.1.112export CP2_IP=192.168.1.113 4. 关闭防火墙、关闭swap、关闭SELinux、调整内核参数(所有节点)1234567891011sudo systemctl stop firewalldsudo systemctl disable firewalldsudo swapoff -asudo sed -i &apos;/ swap / s/^\\(.*\\)$/#\\1/g&apos; /etc/fstabsudo setenforce 0sed -i &apos;s/SELINUX=permissive/SELINUX=disabled/&apos; /etc/sysconfig/selinuxcat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.confnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOFsysctl --system 5.准备镜像（所有节点）1234567891011121314# 创建脚本加入如下内容#!/bin/bashimages=(kube-proxy-amd64:v1.11.1 kube-scheduler-amd64:v1.11.1 kube-controller-manager-amd64:v1.11.1 kube-apiserver-amd64:v1.11.1etcd-amd64:3.2.18 pause-amd64:3.1 kubernetes-dashboard-amd64:v1.8.3 k8s-dns-sidecar-amd64:1.14.9 k8s-dns-kube-dns-amd64:1.14.9k8s-dns-dnsmasq-nanny-amd64:1.14.9 )for imageName in $&#123;images[@]&#125; ; dodocker pull mirrorgooglecontainers/$imageNamedocker tag mirrorgooglecontainers/$imageName k8s.gcr.io/$imageNamedocker rmi mirrorgooglecontainers/$imageNamedonedocker tag da86e6ba6ca1 k8s.gcr.io/pause:3.1docker pull coredns/coredns:1.1.3docker tag coredns/coredns:1.1.3 k8s.gcr.io/coredns:1.1.3docker rmi coredns/coredns:1.1.3 12345# 添加运行权限chmod -R 777 ./xxx.sh# 执行脚本./xxx.sh 6. 安装、配置kubelet （所有节点）1234567891011cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/enabled=1gpgcheck=1repo_gpgcheck=1gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOFyum install -y kubelet-1.11.1 kubeadm-1.11.1 kubectl-1.11.1systemctl enable kubelet 7.准备SSH Keys（任意Master节点）12345# 生成SSH Key（通常在第一台Master上操作，可以在终端上操作）ssh-keygen -t rsa -b 2048 -f /root/.ssh/id_rsa -N &quot;&quot;# 将SSH Key复制给其他主机for host in &#123;$CP0_HOSTNAME,$CP1_HOSTNAME,$CP2_HOSTNAME&#125;; do ssh-copy-id $host; done $host; done 8. 部署第一个keepalived （第一个Master节点）keepalived用于生产浮动的虚拟IP，并将浮动IP分配给优先级最高且haproxy正常运行的节点在第一台Master上配置和启动keepalived，若网卡名称不为示例中的eth0则改为对应名称 1234567891011121314151617181920212223242526272829303132# 安装 部署第一个keepalivedyum install -y keepalived curl psmisc &amp;&amp; systemctl enable keepalived# 自定义配置cat &lt;&lt; EOF &gt; /etc/keepalived/keepalived.confvrrp_script haproxy-check &#123; script &quot;killall -0 haproxy&quot; interval 2 weight 20&#125; vrrp_instance haproxy-vip &#123; state BACKUP priority 102 interface ens33 virtual_router_id 51 advert_int 3 unicast_src_ip $CP0_IP unicast_peer &#123; $CP1_IP $CP2_IP &#125; virtual_ipaddress &#123; $VIP_IP &#125; track_script &#123; haproxy-check weight 20 &#125;&#125;EOF# 启动 keepalivedsystemctl start keepalived 9. 部署第二个keepalived （第二个Master节点）1234567891011121314151617181920212223242526272829303132# 安装 部署第一个keepalivedyum install -y keepalived curl psmisc &amp;&amp; systemctl enable keepalived# 自定义配置cat &lt;&lt; EOF &gt; /etc/keepalived/keepalived.confvrrp_script haproxy-check &#123; script &quot;killall -0 haproxy&quot; interval 2 weight 20&#125;vrrp_instance haproxy-vip &#123; state BACKUP priority 101 interface ens33 virtual_router_id 51 advert_int 3 unicast_src_ip $CP1_IP unicast_peer &#123; $CP0_IP $CP2_IP &#125; virtual_ipaddress &#123; $VIP_IP &#125; track_script &#123; haproxy-check weight 20 &#125;&#125;EOF# 启动 keepalivedsystemctl start keepalived 10. 部署第三个keepalived （第三个Master节点）1234567891011121314151617181920212223242526272829303132# 安装 部署第一个keepalivedyum install -y keepalived curl psmisc &amp;&amp; systemctl enable keepalived# 自定义配置cat &lt;&lt; EOF &gt; /etc/keepalived/keepalived.confvrrp_script haproxy-check &#123; script &quot;killall -0 haproxy&quot; interval 2 weight 20&#125;vrrp_instance haproxy-vip &#123; state BACKUP priority 100 interface ens33 virtual_router_id 51 advert_int 3 unicast_src_ip $CP2_IP unicast_peer &#123; $CP0_IP $CP1_IP &#125; virtual_ipaddress &#123; $VIP_IP &#125; track_script &#123; haproxy-check weight 20 &#125;&#125;EOF# 启动 keepalivedsystemctl start keepalived 11. 部署HAProxy （所有Master节点）12345678910111213141516171819202122232425262728293031323334353637383940414243# 安装HAproxyyum install -y haproxy &amp;&amp; systemctl enable haproxy# 自定义配置文件cat &lt;&lt; EOF &gt; /etc/haproxy/haproxy.cfgglobal log 127.0.0.1 local0 log 127.0.0.1 local1 notice tune.ssl.default-dh-param 2048defaults log global mode http option dontlognull timeout connect 5000ms timeout client 600000ms timeout server 600000mslisten stats bind :9090 mode http balance stats uri /haproxy_stats stats auth admin:admin stats admin if TRUEfrontend kube-apiserver-https mode tcp bind :8443 default_backend kube-apiserver-backendbackend kube-apiserver-backend mode tcp balance roundrobin stick-table type ip size 200k expire 30m stick on src server k8s-master1 192.168.1.111:6443 check server k8s-master2 192.168.1.112:6443 check server k8s-master3 192.168.1.113:6443 checkEOF# 启动 HAproxysystemctl start haproxy 安装kubernetes1. 配置第一个Master节点（第一个Master节点）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253cat &gt;kubeadm-master.config&lt;&lt;EOFapiVersion: kubeadm.k8s.io/v1alpha2kind: MasterConfigurationkubernetesVersion: v1.11.1apiServerCertSANs:- &quot;k8s-master1&quot;- &quot;k8s-master2&quot;- &quot;k8s-master3&quot;- &quot;192.168.1.111&quot;- &quot;192.168.1.112&quot;- &quot;192.168.1.113&quot;- &quot;192.168.1.110&quot;- &quot;127.0.0.1&quot;api: advertiseAddress: $CP0_IP controlPlaneEndpoint: 192.168.1.110:8443etcd: local: extraArgs: listen-client-urls: &quot;https://127.0.0.1:2379,https://$CP0_IP:2379&quot; advertise-client-urls: &quot;https://$CP0_IP:2379&quot; listen-peer-urls: &quot;https://$CP0_IP:2380&quot; initial-advertise-peer-urls: &quot;https://$CP0_IP:2380&quot; initial-cluster: &quot;$CP0_HOSTNAME=https://$CP0_IP:2380&quot; serverCertSANs: - $CP0_HOSTNAME - $CP0_IP peerCertSANs: - $CP0_HOSTNAME - $CP0_IPcontrollerManagerExtraArgs: node-monitor-grace-period: 10s pod-eviction-timeout: 10snetworking: podSubnet: 10.244.0.0/16EOF# 初始化# 注意保存返回的 join 命令kubeadm init --config kubeadm-master.config# 打包ca相关文件上传至其他master节点cd /etc/kubernetes &amp;&amp; tar cvzf k8s-key.tgz admin.conf pki/ca.* pki/sa.* pki/front-proxy-ca.* pki/etcd/ca.*scp k8s-key.tgz k8s-master2:~/scp k8s-key.tgz k8s-master3:~/ssh k8s-master2 &apos;tar xf k8s-key.tgz -C /etc/kubernetes/&apos;ssh k8s-master3 &apos;tar xf k8s-key.tgz -C /etc/kubernetes/&apos; 2. 配置第二个Master节点（第二个Master节点）12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758cat &gt;kubeadm-master.config&lt;&lt;EOFapiVersion: kubeadm.k8s.io/v1alpha2kind: MasterConfigurationkubernetesVersion: v1.11.1apiServerCertSANs:- &quot;k8s-master1&quot;- &quot;k8s-master2&quot;- &quot;k8s-master3&quot;- &quot;192.168.1.111&quot;- &quot;192.168.1.112&quot;- &quot;192.168.1.113&quot;- &quot;192.168.1.110&quot;- &quot;127.0.0.1&quot;api: advertiseAddress: $CP1_IP controlPlaneEndpoint: 192.168.1.110:8443etcd: local: extraArgs: listen-client-urls: &quot;https://127.0.0.1:2379,https://$CP1_IP:2379&quot; advertise-client-urls: &quot;https://$CP1_IP:2379&quot; listen-peer-urls: &quot;https://$CP1_IP:2380&quot; initial-advertise-peer-urls: &quot;https://$CP1_IP:2380&quot; initial-cluster: &quot;$CP0_HOSTNAME=https://$CP0_IP:2380,$CP1_HOSTNAME=https://$CP1_IP:2380&quot; initial-cluster-state: existing serverCertSANs: - $CP1_HOSTNAME - $CP1_IP peerCertSANs: - $CP1_HOSTNAME - $CP1_IPcontrollerManagerExtraArgs: node-monitor-grace-period: 10s pod-eviction-timeout: 10snetworking: podSubnet: 10.244.0.0/16EOF# 配置kubeletkubeadm alpha phase certs all --config kubeadm-master.configkubeadm alpha phase kubelet config write-to-disk --config kubeadm-master.configkubeadm alpha phase kubelet write-env-file --config kubeadm-master.configkubeadm alpha phase kubeconfig kubelet --config kubeadm-master.configsystemctl restart kubelet# 添加etcd到集群中KUBECONFIG=/etc/kubernetes/admin.conf kubectl exec -n kube-system etcd-$&#123;CP0_HOSTNAME&#125; -- etcdctl --ca-file /etc/kubernetes/pki/etcd/ca.crt --cert-file /etc/kubernetes/pki/etcd/peer.crt --key-file /etc/kubernetes/pki/etcd/peer.key --endpoints=https://$&#123;CP0_IP&#125;:2379 member add $&#123;CP1_HOSTNAME&#125; https://$&#123;CP1_IP&#125;:2380kubeadm alpha phase etcd local --config kubeadm-master.config# 部署kubeadm alpha phase kubeconfig all --config kubeadm-master.configkubeadm alpha phase controlplane all --config kubeadm-master.configkubeadm alpha phase mark-master --config kubeadm-master.config 3. 部署第三个Master节点 （第三个Master节点）12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758cat &gt;kubeadm-master.config&lt;&lt;EOFapiVersion: kubeadm.k8s.io/v1alpha2kind: MasterConfigurationkubernetesVersion: v1.11.1apiServerCertSANs:- &quot;k8s-master1&quot;- &quot;k8s-master2&quot;- &quot;k8s-master3&quot;- &quot;192.168.1.111&quot;- &quot;192.168.1.112&quot;- &quot;192.168.1.113&quot;- &quot;192.168.1.110&quot;- &quot;127.0.0.1&quot;api: advertiseAddress: $CP2_IP controlPlaneEndpoint: 192.168.1.110:8443etcd: local: extraArgs: listen-client-urls: &quot;https://127.0.0.1:2379,https://$CP2_IP:2379&quot; advertise-client-urls: &quot;https://$CP2_IP:2379&quot; listen-peer-urls: &quot;https://$CP2_IP:2380&quot; initial-advertise-peer-urls: &quot;https://$CP2_IP:2380&quot; initial-cluster: &quot;$CP0_HOSTNAME=https://$CP0_IP:2380,$CP1_HOSTNAME=https://$CP1_IP:2380,$CP2_HOSTNAME=https://$CP2_IP:2380&quot; initial-cluster-state: existing serverCertSANs: - $CP2_HOSTNAME - $CP2_IP peerCertSANs: - $CP2_HOSTNAME - $CP2_IPcontrollerManagerExtraArgs: node-monitor-grace-period: 10s pod-eviction-timeout: 10snetworking: podSubnet: 10.244.0.0/16EOF# 配置kubeletkubeadm alpha phase certs all --config kubeadm-master.configkubeadm alpha phase kubelet config write-to-disk --config kubeadm-master.configkubeadm alpha phase kubelet write-env-file --config kubeadm-master.configkubeadm alpha phase kubeconfig kubelet --config kubeadm-master.configsystemctl restart kubelet# 添加etcd到集群中KUBECONFIG=/etc/kubernetes/admin.conf kubectl exec -n kube-system etcd-$&#123;CP0_HOSTNAME&#125; -- etcdctl --ca-file /etc/kubernetes/pki/etcd/ca.crt --cert-file /etc/kubernetes/pki/etcd/peer.crt --key-file /etc/kubernetes/pki/etcd/peer.key --endpoints=https://$&#123;CP0_IP&#125;:2379 member add $&#123;CP2_HOSTNAME&#125; https://$&#123;CP2_IP&#125;:2380kubeadm alpha phase etcd local --config kubeadm-master.config# 部署kubeadm alpha phase kubeconfig all --config kubeadm-master.configkubeadm alpha phase controlplane all --config kubeadm-master.configkubeadm alpha phase mark-master --config kubeadm-master.config 4. 配置使用kubectl (在任意master节点操作)123456789101112rm -rf $HOME/.kubemkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config# 查看node节点kubectl get nodes# 只有网络插件也安装配置完成之后，才能会显示为ready状态# 设置master允许部署应用pod，参与工作负载，现在可以部署其他系统组件# 如 dashboard, heapster, efk等kubectl taint nodes --all node-role.kubernetes.io/master- 5. 配置网络插件 (在任意master节点操作)123curl -O https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.ymlkubectl apply -f kube-flannel.yml 6. 子节点加入集群 （所有子节点）就是你初始化第一个master保存的命令。1kubeadm join k8s.test.local:8443 --token bqnani.kwxe3y34vy22xnhm --discovery-token-ca-cert-hash sha256:b6146fea7a63d3a66e406c12f55f8d99537db99880409939e4aba206300e06cc 7. 查看节点状态1kubectl get nodes 返回如下：123456NAME STATUS ROLES AGE VERSIONk8s-master1 Ready master 49m v1.11.1k8s-master2 Ready master 47m v1.11.1k8s-master3 Ready master 45m v1.11.1k8s-slave1 Ready &lt;none&gt; 42m v1.11.1k8s-slave2 Ready &lt;none&gt; 42m v1.11.1 8. 配置k8s可视化没有Git请自行安装。 1234567# clone 配置文件git clone https://github.com/BrickCat/kubernetes-dashboard.git# 执行文件cd kubernetes-dashboardkubectl -n kube-system create -f . 配置角色文件1vim dashboard-admin.yaml 添加如下内容：1234567891011121314apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: kubernetes-dashboard labels: k8s-app: kubernetes-dashboardroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects:- kind: ServiceAccount name: kubernetes-dashboard namespace: kube-system 1kubectl -f ./dashboard-admin.yaml create 9. 访问集群 http://192.168.1.111:30090","tags":[{"name":"容器","slug":"容器","permalink":"https://blog.githink.cn/tags/容器/"},{"name":"kubernetes","slug":"kubernetes","permalink":"https://blog.githink.cn/tags/kubernetes/"},{"name":"k8s","slug":"k8s","permalink":"https://blog.githink.cn/tags/k8s/"},{"name":"docker","slug":"docker","permalink":"https://blog.githink.cn/tags/docker/"}]},{"title":"Spring cloud config server 使用本地配置文件","date":"2018-02-09T14:32:36.000Z","path":"2018/02/09/Spring-cloud-config-server-使用本地配置文件/","text":"为了解决有的时候使用Git作为Config Server托管中心拉取配置失败的问题 学习Spring Cloud有一段时间了，今天在码云上看见一个非常不错的 Spring Cloud的项目 Pig ,功能十分强大，配套教程、文档也特别全。这么牛逼的项目不得down下来学习学习？于是clone下来，跟着视频安装好了各种环境，本以为万无一失的挨个启动就可以了。没想到启动到Config Server的时候并没有从Git服务器上拉取到配置信息。也去群里询问了，也问了作者 @冷冷 搞了半天也不行。后来觉得可能是公司的网管为了信息安全把一些端口给禁掉了，所有就想到把配置文件本地化了。 下面我们就来看看如何将配置文件本地化。 首先，我们先把配置文件从自己的Git服务器上拉取到本地。 12git clone https://gitee.com/cqzqxq_lxh/pig-config.gitgit checkout dev 然后，拉取Spring Cloud 项目。 1git clone https://gitee.com/log4j/pig.git 在pig-config模块下创建文件夹，把刚刚clone下来的配置文件拷贝到 properties文件夹中。 最后修改pig-config模块下的配置文件 application.yml。 注意：项目的启动腰包所有的配置文件的端口、Ip、用户名、密码之类的配置要改成自己的。 以上。","tags":[{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"https://blog.githink.cn/tags/Spring-Cloud/"}]},{"title":"axios post提交参数java后台接收不到参数解决办法","date":"2017-11-08T05:37:31.000Z","path":"2017/11/08/post传参/","text":"问题描述在浏览器中使用axiosPOST提交数据在后台接收的时候接收不到。 解决办法 首先我们先来了解一下什么是axios Axios 是一个基于 promise 的 HTTP 库，可以用在浏览器和 node.js 中。 问题原因 由于axios默认发送数据时，数据格式是Request Payload，而并非我们常用的Form Data格式，所以Java后台获取不到数据。在发送之前需要对数据进行处理。 解决办法12345678910var params = new URLSearchParams();params.append('firstName',Fred);axios.post('/user', params) .then(function (response) &#123; console.log(response); &#125;) .catch(function (error) &#123; console.log(error); &#125;);","tags":[{"name":"axios","slug":"axios","permalink":"https://blog.githink.cn/tags/axios/"},{"name":"javascript","slug":"javascript","permalink":"https://blog.githink.cn/tags/javascript/"},{"name":"常见问题","slug":"常见问题","permalink":"https://blog.githink.cn/tags/常见问题/"}]},{"title":"解决springboot-shiro-session设置过期时间无效的问题","date":"2017-10-30T06:19:19.000Z","path":"2017/10/30/springboot-shiro-session/","text":"问题描述在springboot整合shiro的时候session总是过期，往往是刚刚登录没一两分钟就又要重新登陆。网上查了很多资料说是可以设置session的过期时间server.session.timeout，无论你设置成多少都无效，后来才知道此session和shiro的session根本就是两回事。 解决办法下边我们就来看看在springboot中如何设置shiro的session过期时间。废话不多说直接上代码吧！ 1234567891011121314151617181920212223242526272829303132333435363738 /** * 添加session管理器 * @return */@Bean(name=\"sessionManager\")public DefaultWebSessionManager defaultWebSessionManager() &#123; System.out.println(\"ShiroConfiguration.defaultWebSessionManager()\"); DefaultWebSessionManager sessionManager = new DefaultWebSessionManager(); sessionManager.setCacheManager(cacheManager()); sessionManager.setSessionValidationInterval(3600000*12); sessionManager.setGlobalSessionTimeout(3600000*12); sessionManager.setDeleteInvalidSessions(true); sessionManager.setSessionValidationSchedulerEnabled(true); Cookie cookie = new SimpleCookie(ShiroHttpSession.DEFAULT_SESSION_ID_NAME); cookie.setName(\"ITBC\"); cookie.setHttpOnly(true); sessionManager.setSessionIdCookie(cookie); return sessionManager;&#125;/** * 用户授权信息Cache */ @Bean(name = \"shiroCacheManager\") @ConditionalOnMissingBean public CacheManager cacheManager() &#123; return new MemoryConstrainedCacheManager(); &#125; @Bean(name = \"securityManager\") @ConditionalOnMissingBean public DefaultSecurityManager securityManager() &#123; DefaultSecurityManager sm = new DefaultWebSecurityManager(); //添加缓存 sm.setCacheManager(cacheManager()); //添加session sm.setSessionManager(defaultWebSessionManager()); return sm; &#125; 后记其实在springboot的实际开发中博主遇到了很多坑，网上的解决办法也是零零碎碎的，总感觉不是自己想要的。有时间整理整理总结一下，发出来大家一起学习。","tags":[{"name":"常见问题","slug":"常见问题","permalink":"https://blog.githink.cn/tags/常见问题/"},{"name":"springboot","slug":"springboot","permalink":"https://blog.githink.cn/tags/springboot/"},{"name":"shiro","slug":"shiro","permalink":"https://blog.githink.cn/tags/shiro/"}]},{"title":"安装node以及npm学习ant-desgin","date":"2017-09-20T01:20:18.000Z","path":"2017/09/20/安装node以及npm学习ant-desgin/","text":"学习ant-desgin遇到的一些问题 在Mac上用brew安装的node.js,在使用npm时有些包是下载不下来的，导致项目编译失败。解决办法就是去官网下载LTS版本的node.js。 1、首先先把Mac上的npm下载的包删干净，然后在执行 brew uninstall node 参考 2、下载node.js 网址：https://nodejs.org/en/ 注意：下载LTS版本 3、进入到你项目的目录里，再次执行 npm i","tags":[{"name":"react","slug":"react","permalink":"https://blog.githink.cn/tags/react/"},{"name":"ant-desgin","slug":"ant-desgin","permalink":"https://blog.githink.cn/tags/ant-desgin/"},{"name":"dva","slug":"dva","permalink":"https://blog.githink.cn/tags/dva/"},{"name":"npm","slug":"npm","permalink":"https://blog.githink.cn/tags/npm/"},{"name":"node","slug":"node","permalink":"https://blog.githink.cn/tags/node/"}]},{"title":"安装 Semantic UI","date":"2017-09-02T15:43:05.000Z","path":"2017/09/02/Semantic UI/","text":"Semantic UI Semantic作为一款开发框架，帮助开发者使用对人类友好的HTML语言构建优雅的响应式布局。它还提供了一套很方便的定制主题的方法，你可以用自己的想法去改变界面组件的样式。在这个教程里我们学习一下安装 Semantic UI 。 准备工具 我们使用shell去安装semantic UI，前提是你已经安装好了nmp gulp。 开始安装 创建一个目录 12mkdir semanticcd semantic 用npm安装semantic UI 1npm install semantic-ui 一路回车就OK了。 查看是否安装成功1234├── node_modules├── package-lock.json├── semantic└── semantic.json 这是安装之后的文件夹和文件列表。 编译 进入到semantic的目录里（是安装完之后的semantic的目录） 然后执行 gulp 命令。 12 cd semantic gulp bulid 查看编译后的文件编译完之后会在文件夹中生成dist文件夹。 1dist gulpfile.js src tasks 现在到dist文件夹里看看都生成了什么东吧~ 12components semantic.js semantic.min.jssemantic.css semantic.min.css themes components 目录下面是单独的一些组件，如果你只想使用 Semantic UI 里的某些组件，可以在这个目录下面找到这些组件。如果你想使用全部的组件，可以使用 semantic.css 与 semantic.js ，或者使用它们的最小化之后的版本，semanitc.min.css 与 semantic.min.js 。 themes 目录下是主题的样式，现在主要有四种主题:1basic default github material 修改主题只需要进入到src文件夹中修改theme.config文件即可。例如把default换成github。","tags":[{"name":"Semantic UI","slug":"Semantic-UI","permalink":"https://blog.githink.cn/tags/Semantic-UI/"}]},{"title":"windows 搭建Git服务器","date":"2017-08-31T03:42:59.000Z","path":"2017/08/31/windows搭建Git服务器/","text":"初衷 由于在GitHub上的私有项目是付费的，并且用GitHub Pages 搭建Hexo博客有的时候慢的真的想砸了电脑。所以就想自己搭建一个Git服务器，用于放一些私有项目和搭建一个Hexo博客。 下面就跟着我一起来用最简单的方法搭建Git服务器吧~ 下载相关软件 gitstack 安装 下载完后双击就可以，安装的过程中是默认用80端口的，先把占用80端口的进程Kill掉。重新启动程序。一路下一步就好。安装的过程中也可以自定义安装目录。安装完成后在浏览器中输入http://localhost/gitstack就可以访问了，默认用户名密码是admin/admin。 配置基本配置 修改密码：在Settings-&gt;general-&gt;Administrator password中修改默认的密码。 修改端口：在Settings-&gt;general-&gt;Server Ports中修改端口号，后续访问的话就是IP地址加端口号访问。 修改仓库地址：Settings-&gt;general-&gt;Repositories Location中修改该地址。 添加用户和组 添加用户 添加组 创建Repositories 给Repositories加用户 创建完Repositories要给它添加用户的，否则访问不了。注意：管理员账户并不能访问这个Repositories。 最后 git clone 你创建的Repositories，提示你输入用户名密码，就能把项目clone下来。 预告搭建不依赖于GitHub Pages 的 Hexo 博客。","tags":[{"name":"Git","slug":"Git","permalink":"https://blog.githink.cn/tags/Git/"},{"name":"server","slug":"server","permalink":"https://blog.githink.cn/tags/server/"}]},{"title":"windows搭建流媒体服务器","date":"2017-08-30T03:42:59.000Z","path":"2017/08/30/windows搭建流媒体服务器/","text":"理论什么是流媒体服务器 流媒体指以流方式在网络中传送音频、视频和多媒体文件的媒体形式。相对于下载后观看的网络播放形式而言，流媒体的典型特征是把连续的音频和视频信息压缩后放到网络服务器上，用户边下载边观看，而不必等待整个文件下载完毕。由于流媒体技术的优越性，该技术广泛应用于视频点播、视频会议、远程教育、远程医疗和在线直播系统中。 作为新一代互联网应用的标志，流媒体技术在近几年得到了飞速的发展。流媒体服务器是流媒体应用的核心系统，是运营商向用户提供视频服务的关键平台。流媒体服务器的主要功能是对流媒体内容进行采集、缓存、调度和传输播放。流媒体应用系统的主要性能体现都取决于媒体服务器的性能和服务质量。因此，流媒体服务器是流媒体应用系统的基础，也是最主要的组成部分。 什么是HLS (HTTP Live Streaming) 常用的流媒体协议主要有 HTTP 渐进下载和基于 RTSP/RTP 的实时流媒体协议，这二种基本是完全不同的东西，目前比较方便又好用的是用 HTTP 渐进下载方法。在这个中 apple 公司的 HTTP Live Streaming 是这个方面的代表。它最初是苹果公司针对iPhone、iPod、iTouch和iPad等移动设备而开发的流.现在见到在桌面也有很多应用了，HTML5 是直接支持这个。但是HLS协议的小切片方式会生成大量的文件，存储或处理这些文件会造成大量资源浪费。如果要实现数天的时移，索引量将会是个巨额数字，并明显影响请求速度。因此，HLS协议对存储I/O要求相当苛刻。对此，也有公司提出了非常好的解决方案。 新型点播服务器系统，独创了内存缓存数据实时切片技术，颠覆了这种传统实现方法，从根本上解决了大量切片的碎片问题，使得单台服务器的切片与打包能力不再是瓶颈。其基本原理如下： 不将TS切片文件存到磁盘，而是存在内存当中，这种技术使得服务器的磁盘上面不再会有“数以吨计”的文件碎片，极大减少了磁盘的I/O次数，延长了服务器磁盘的使用寿命，极大提高了服务器运行的稳定性。同时，由于使用这种技术，使得终端请求数据时直接从服务器的内存中获取，极大提高了对终端数据请求的反应速度，优化了视频观看体验。 什么是FFmpeg FFmpeg是一套可以用来记录、转换数字音频、视频，并能将其转化为流的开源计算机程序。采用LGPL或GPL许可证。它提供了录制、转换以及流化音视频的完整解决方案。它包含了非常先进的音频/视频编解码库libavcodec，为了保证高可移植性和编解码质量，libavcodec里很多code都是从头开发的。 什么是Nginx Nginx (engine x) 是一个高性能的HTTP和反向代理服务器，也是一个IMAP/POP3/SMTP服务器。 实践安装 JAVA环境不在这里赘述，自行百度。博主这里是Windows server 2012的环境，后续会出Mac、Linux环境的教程。 安装NginxNginx-1.12.0 stable 查看Nginx的版本号：nginx -v 启动Nginx: cd 安装目录 ，start nginx 快速停止或关闭Nginx：nginx -s stop 正常停止或关闭Nginx：nginx -s quit 配置文件修改重装载命令：nginx -s reload 安装FFmpegFFmpeg 根据自己的操作系统选择下载最新的32位或64位静态程序版本。 配置配置nginx 博主感觉Nginx真的是一个好东西，能做点播服务器、直播服务器、文件服务器、负载均衡等等一些牛逼的功能。深入了解只有不得不佩服它的的强大。下面就跟着博主一起去见识一下Nginx吧（后续博主会写Nginx配置https和wss以及一个IP绑定多个域名等）。 直接解压缩就好了，windows下直接点击nginx.exe就可以启动Nginx，在浏览器里访问http://localhost就可以看到Nigix的欢迎页面。由于Nginx默认使用80端口可能会被占用。你要先kill掉占用80端口的进程。 在conf文件夹中打开mine.types文件。 在application/zip zip;后面加上如下两行： 12application/x-mpegURL m3u8; application/vnd.apple.mpegurl m3u8; 然后在video/x-msvideo avi;添加video/MP2T ts; 打开nginx.conf文件，（最好先备份一下） 下面就是如何配置Nginx的代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134#user nobody;worker_processes 1;#要开启的进程数 一般等于cpu的总核数 其实一般情况下开4个或8个就可 我开2个#error_log logs/error.log;#error_log logs/error.log notice;#error_log logs/error.log info;#pid logs/nginx.pid;events &#123; worker_connections 1024;#默认最大的并发数为1024，如果你的网站访问量过大，已经远远超过1024这个并发数，那你就要修改worker_connecions这个值 ，这个值越大，并发数也有就大。当然，你一定要按照你自己的实际情况而定，也不能设置太大，不能让你的CPU跑满100%。&#125;http &#123; include mime.types; #解决Nginx跨域访问 Begin default_type application/octet-stream; add_header Cache-Control no-cache; add_header 'Access-Control-Allow-Origin' '*' always; add_header 'Access-Control-Expose-Headers' 'Content-Length,Content-Range'; add_header 'Access-Control-Allow-Headers' 'Range'; #解决Nginx跨域访问 End #log_format main '$remote_addr - $remote_user [$time_local] \"$request\" ' # '$status $body_bytes_sent \"$http_referer\" ' # '\"$http_user_agent\" \"$http_x_forwarded_for\"'; #access_log logs/access.log main; sendfile on; #tcp_nopush on; #keepalive_timeout 0; keepalive_timeout 65; #gzip on; server &#123; listen 8080;#监听的的端口默认是80 server_name localhost;#默认就行 #charset koi8-r; #access_log logs/host.access.log main; #请求路径 / location / &#123; #设置HTTP Response的Content-Type types&#123; application/vnd.apple.mpegurl m3u8; video/MP2T ts; &#125; #指定访问的根目录，也是放你视频切片文件的地方，不用配置可直接访问 如：http://localhost:8080/playList.m3u8 root html; &#125; #访问.mp4格式的文件 location ~ .mp4 &#123; mp4; &#125; #error_page 404 /404.html; # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; # proxy the PHP scripts to Apache listening on 127.0.0.1:80 # #location ~ \\.php$ &#123; # proxy_pass http://127.0.0.1; #&#125; # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000 # #location ~ \\.php$ &#123; # root html; # fastcgi_pass 127.0.0.1:9000; # fastcgi_index index.php; # fastcgi_param SCRIPT_FILENAME /scripts$fastcgi_script_name; # include fastcgi_params; #&#125; # deny access to .htaccess files, if Apache's document root # concurs with nginx's one # #location ~ /\\.ht &#123; # deny all; #&#125; &#125; # another virtual host using mix of IP-, name-, and port-based configuration # #server &#123; # listen 8000; # listen somename:8080; # server_name somename alias another.alias; # location / &#123; # root html; # index index.html index.htm; # &#125; #&#125; # HTTPS server # #server &#123; # listen 443 ssl; # server_name localhost; # ssl_certificate cert.pem; # ssl_certificate_key cert.key; # ssl_session_cache shared:SSL:1m; # ssl_session_timeout 5m; # ssl_ciphers HIGH:!aNULL:!MD5; # ssl_prefer_server_ciphers on; # location / &#123; # root html; # index index.html index.htm; # &#125; #&#125; &#125; 到这里Nginx全部配置完成。 配置FFmpeg配置环境变量 解压缩后它会生成一个类似名为“ffmpeg-20150504-Git-eb9fb50-win32-static”的新文件夹，重命名为ffmpeg 配置 配置环境变量点击“开始菜单”，再点击“控制面板”，再点击“系统与安全”，再点击“系统”，然后点击“高级系统设置”，跳出“系统属性”窗口后，最后点击“环境变量”按钮：点击“环境变量”按钮后，跳出“环境变量”窗口，找到并选中“Path”变量，点击编辑：在“Path”变量原有变量值内容上加上“;d:\\ffmpeg\\bin”（注：;代表间隔，不可遗漏；d:\\ffmpeg\\bin代表FFmpeg的安装路径下的bin文件夹），一路点击“确定”即可。打开命令提示符窗口。输入命令“ffmpeg –version”。如果命令提示窗口返回FFmpeg的版本信息，那么就说明安装成功了，你可以在命令提示行中任意文件夹下运行FFmpeg。 视频切片与访问 命令： 1ffmpeg -i output.mp4 -c:v libx264 -c:a aac -strict -2 -f hls -hls_list_size 0 -hls_time 5 output.m3u8 输入与输出的文件都可以添加路径。如：ffmpeg -i C:\\Users\\Desktop\\output.mp4 -c:v libx264 -c:a aac -strict -2 -f hls -hls_list_size 0 -hls_time 5 C:\\Users\\Desktop\\nginx-1.12.0\\html\\output.m3u8-hls_list_size n:设置播放列表保存的最多条目，设置为0会保存有所片信息，默认值为5。-hls_time n: 设置每片的长度，默认值为2。单位为秒。 查看下载VLC media player打开本地文件output.m3u8,如果能播放表示切片成功。 Nginx结合FFmpeg 大体思路就是把视频切片到指定位置，用nginx去代理你这个位置，就可以了。 一 把nginx文件里的html文件夹下的所有文件都删除。新建一个m3u8的文件夹。 二 把切片命令的输出地址换成你的m3u8文件夹所在的位置。 三 启动Nginx,在VLC中输入http://localhost:8080/m3u8/output.m3u8（nginx监听的8080端口） 预告 如何用Webuploader上传大文件视频，并用FFmpeg切片，Nginx代理文件，html播放。","tags":[{"name":"server","slug":"server","permalink":"https://blog.githink.cn/tags/server/"},{"name":"nginx","slug":"nginx","permalink":"https://blog.githink.cn/tags/nginx/"},{"name":"ffmpeg","slug":"ffmpeg","permalink":"https://blog.githink.cn/tags/ffmpeg/"},{"name":"hls","slug":"hls","permalink":"https://blog.githink.cn/tags/hls/"}]}]